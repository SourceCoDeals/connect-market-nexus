import { createClient } from 'https://esm.sh/@supabase/supabase-js@2';
import { validateUrl, ssrfErrorResponse } from "../_shared/security.ts";
import { logAICallCost } from "../_shared/cost-tracker.ts";
import { type RateLimitConfig } from "../_shared/ai-providers.ts";
import { withConcurrencyTracking, reportRateLimit } from "../_shared/rate-limiter.ts";
import { getCorsHeaders, corsPreflightResponse } from "../_shared/cors.ts";
import { type SourceType, validateFieldProvenance } from "../_shared/provenance.ts";
import {
  // Configuration
  BUYER_AI_CONFIG,
  BUYER_MIN_CONTENT_LENGTH,
  BUYER_SCRAPE_TIMEOUT_MS,
  BUYER_AI_CONCURRENCY,
  // Constants
  LOCATION_PATTERNS,
  FIELD_TO_COLUMN_MAP,
  // Extraction functions
  extractBusinessOverview,
  extractCustomerProfile,
  extractGeography,
  extractPEIntelligence,
  // Validation
  validateGeography,
  // Update building
  buildBuyerUpdateObject,
  // Types
  type AIExtractionResult,
} from "../_shared/buyer-extraction.ts";

// ============================================================================
// FIRECRAWL SCRAPING
// ============================================================================

// Module-level rate limit config â€” set once per invocation from the main handler's supabase client
let _rateLimitConfig: RateLimitConfig | undefined;

async function scrapeWebsite(url: string, apiKey: string): Promise<{ success: boolean; content?: string; error?: string }> {
  const doScrape = async () => {
    let formattedUrl = url.trim();
    if (!formattedUrl.startsWith('http://') && !formattedUrl.startsWith('https://')) {
      formattedUrl = `https://${formattedUrl}`;
    }

    const response = await fetch('https://api.firecrawl.dev/v1/scrape', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        url: formattedUrl,
        formats: ['markdown'],
        onlyMainContent: true,
        waitFor: 1500,
      }),
      signal: AbortSignal.timeout(BUYER_SCRAPE_TIMEOUT_MS),
    });

    if (response.status === 429 && _rateLimitConfig?.supabase) {
      await reportRateLimit(_rateLimitConfig.supabase, 'firecrawl');
    }

    if (!response.ok) {
      return { success: false, error: `HTTP ${response.status}` };
    }

    const data = await response.json();
    const content = data.data?.markdown || data.markdown || '';

    if (!content || content.length < BUYER_MIN_CONTENT_LENGTH) {
      return { success: false, error: `Insufficient content (${content.length} chars)` };
    }

    return { success: true, content };
  };

  try {
    if (_rateLimitConfig?.supabase) {
      return await withConcurrencyTracking(_rateLimitConfig.supabase, 'firecrawl', doScrape);
    }
    return await doScrape();
  } catch (error) {
    if (error instanceof Error && error.name === 'TimeoutError') {
      return { success: false, error: `Timed out after ${BUYER_SCRAPE_TIMEOUT_MS / 1000}s` };
    }
    return { success: false, error: error instanceof Error ? error.message : 'Unknown error' };
  }
}

async function firecrawlMap(url: string, apiKey: string, limit = 100): Promise<string[]> {
  const doMap = async () => {
    let formattedUrl = url.trim();
    if (!formattedUrl.startsWith('http://') && !formattedUrl.startsWith('https://')) {
      formattedUrl = `https://${formattedUrl}`;
    }

    const response = await fetch('https://api.firecrawl.dev/v1/map', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        url: formattedUrl,
        limit,
        includeSubdomains: false,
      }),
      signal: AbortSignal.timeout(BUYER_SCRAPE_TIMEOUT_MS),
    });

    if (response.status === 429 && _rateLimitConfig?.supabase) {
      await reportRateLimit(_rateLimitConfig.supabase, 'firecrawl');
    }

    const data = await response.json();
    if (!response.ok || !data.success) {
      return [];
    }

    return data.links || [];
  };

  try {
    if (_rateLimitConfig?.supabase) {
      return await withConcurrencyTracking(_rateLimitConfig.supabase, 'firecrawl', doMap);
    }
    return await doMap();
  } catch (error) {
    console.error('Firecrawl map error:', error);
    return [];
  }
}

// ============================================================================
// MAIN HANDLER (ORCHESTRATOR)
// ============================================================================

Deno.serve(async (req) => {
  const corsHeaders = getCorsHeaders(req);

  if (req.method === 'OPTIONS') {
    return corsPreflightResponse(req);
  }

  try {
    console.log('[enrich-buyer] request received');
    const { buyerId, skipLock } = await req.json();

    if (!buyerId) {
      return new Response(
        JSON.stringify({ success: false, error: 'buyerId is required' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const firecrawlApiKey = Deno.env.get('FIRECRAWL_API_KEY');
    const geminiApiKey = Deno.env.get('GEMINI_API_KEY');
    const supabaseUrl = Deno.env.get('SUPABASE_URL');
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY');

    if (!firecrawlApiKey || !geminiApiKey || !supabaseUrl || !supabaseServiceKey) {
      return new Response(
        JSON.stringify({ success: false, error: 'Server configuration error - missing API keys (FIRECRAWL_API_KEY, GEMINI_API_KEY)' }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const supabase = createClient(supabaseUrl, supabaseServiceKey);
    _rateLimitConfig = { supabase };

    // Fetch buyer
    const { data: buyer, error: buyerError } = await supabase
      .from('remarketing_buyers')
      .select('*')
      .eq('id', buyerId)
      .single();

    if (buyerError || !buyer) {
      return new Response(
        JSON.stringify({ success: false, error: 'Buyer not found' }),
        { status: 404, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // Handle multiple comma-separated URLs â€” take the first one
    let platformWebsite = buyer.platform_website || buyer.company_website;
    let peFirmWebsite = buyer.pe_firm_website;
    if (platformWebsite?.includes(',')) {
      platformWebsite = platformWebsite.split(',').map((u: string) => u.trim()).filter(Boolean)[0] || null;
      console.log(`Multiple platform URLs detected, using first: "${platformWebsite}"`);
    }
    if (peFirmWebsite?.includes(',')) {
      peFirmWebsite = peFirmWebsite.split(',').map((u: string) => u.trim()).filter(Boolean)[0] || null;
      console.log(`Multiple PE firm URLs detected, using first: "${peFirmWebsite}"`);
    }

    if (!platformWebsite && !peFirmWebsite) {
      return new Response(
        JSON.stringify({ success: false, error: 'No website URLs provided. Add platform_website or pe_firm_website first.' }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // SSRF validation
    if (platformWebsite) {
      const validation = validateUrl(platformWebsite);
      if (!validation.valid) {
        return ssrfErrorResponse(`Platform website: ${validation.reason}`);
      }
    }
    if (peFirmWebsite) {
      const validation = validateUrl(peFirmWebsite);
      if (!validation.valid) {
        return ssrfErrorResponse(`PE firm website: ${validation.reason}`);
      }
    }

    // ENRICHMENT LOCK: Prevent concurrent enrichments
    // Skip lock when called from queue worker (queue already manages concurrency via status)
    if (!skipLock) {
      const ENRICHMENT_LOCK_SECONDS = 15;
      const lockCutoff = new Date(Date.now() - ENRICHMENT_LOCK_SECONDS * 1000).toISOString();

      const { data: lockCheck } = await supabase
        .from('remarketing_buyers')
        .select('id, data_last_updated')
        .eq('id', buyerId)
        .single();

      const isLocked = lockCheck?.data_last_updated && lockCheck.data_last_updated > lockCutoff;

      if (isLocked) {
        console.log(`[enrich-buyer] Lock active for buyer ${buyerId}: data_last_updated=${lockCheck.data_last_updated}, cutoff=${lockCutoff}`);
        return new Response(
          JSON.stringify({
            success: false,
            error: `Enrichment already in progress for this buyer. Please wait ${ENRICHMENT_LOCK_SECONDS} seconds and try again.`,
            statusCode: 429
          }),
          { status: 429, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }

      // Claim the lock
      await supabase
        .from('remarketing_buyers')
        .update({ data_last_updated: new Date().toISOString() })
        .eq('id', buyerId);
    } else {
      // Queue-based call: just update timestamp without lock check
      await supabase
        .from('remarketing_buyers')
        .update({ data_last_updated: new Date().toISOString() })
        .eq('id', buyerId);
      console.log(`[enrich-buyer] Skipping lock (queue-based call) for buyer ${buyerId}`);
    }

    console.log(`Starting 4-prompt enrichment for buyer: ${buyer.company_name || buyer.pe_firm_name || buyerId}`);
    console.log(`Platform website: ${platformWebsite || 'none'}`);
    console.log(`PE firm website: ${peFirmWebsite || 'none'}`);

    const existingSources = Array.isArray(buyer.extraction_sources) ? buyer.extraction_sources : [];
    const hasTranscriptSource = existingSources.some(
      (src: any) => src.type === 'transcript' || src.type === 'buyer_transcript' || src.source === 'transcript'
    );

    const warnings: string[] = [];
    const sources: { platform?: string; pe_firm?: string } = {};
    let platformContent = '';
    let peContent = '';

    // ========================================================================
    // SCRAPE WEBSITES IN PARALLEL
    // ========================================================================

    const scrapePromises: Promise<{ type: string; result: { success: boolean; content?: string; error?: string } }>[] = [];

    if (platformWebsite) {
      scrapePromises.push(
        scrapeWebsite(platformWebsite, firecrawlApiKey).then(result => ({ type: 'platform', result }))
      );
    }
    if (peFirmWebsite && peFirmWebsite !== platformWebsite) {
      scrapePromises.push(
        scrapeWebsite(peFirmWebsite, firecrawlApiKey).then(result => ({ type: 'peFirm', result }))
      );
    }

    const scrapeResults = await Promise.all(scrapePromises);

    // Start firecrawlMap discovery in parallel â€” but don't block on it
    let locationPagePromise: Promise<string | null> | null = null;

    for (const { type, result } of scrapeResults) {
      if (type === 'platform') {
        if (result.success && result.content) {
          platformContent = result.content;
          sources.platform = platformWebsite!;
          console.log(`Scraped platform website: ${platformContent.length} chars`);

          // PERF: Start location page discovery immediately (runs in parallel with Gemini batch 1)
          locationPagePromise = firecrawlMap(platformWebsite!, firecrawlApiKey)
            .then(async (links) => {
              const locationPage = links.find(link =>
                LOCATION_PATTERNS.some(p => link.toLowerCase().includes(p))
              );
              if (locationPage) {
                console.log(`Found location page: ${locationPage}`);
                const locationResult = await scrapeWebsite(locationPage, firecrawlApiKey);
                if (locationResult.success && locationResult.content) {
                  return locationResult.content;
                }
              }
              return null;
            })
            .catch((err) => {
              console.warn('Location page discovery failed:', err);
              return null;
            });
        } else {
          warnings.push(`Platform website scrape failed: ${result.error}`);
        }
      } else if (type === 'peFirm') {
        if (result.success && result.content) {
          peContent = result.content;
          sources.pe_firm = peFirmWebsite!;
          console.log(`Scraped PE firm website: ${peContent.length} chars`);
        } else {
          warnings.push(`PE firm website scrape failed: ${result.error}`);
        }
      }
    }

    if (!platformContent && !peContent) {
      return new Response(
        JSON.stringify({ success: false, error: 'Could not scrape any website content', warnings }),
        { status: 400, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // ========================================================================
    // RUN EXTRACTION PROMPTS (ALL IN PARALLEL via Gemini Flash)
    // ========================================================================

    const allExtracted: Record<string, any> = {};
    const fieldSourceMap: Record<string, SourceType> = {};
    const evidenceRecords: any[] = [];
    const timestamp = new Date().toISOString();
    let promptsRun = 0;
    let promptsSuccessful = 0;
    let totalInputTokens = 0;
    let totalOutputTokens = 0;
    let billingError: { code: string; message: string } | null = null;
    const provenanceViolations: string[] = [];

    // Determine source type from URL
    const getSourceType = (url: string | null | undefined): SourceType => {
      if (!url) return 'manual';
      const normalizedUrl = url.toLowerCase().replace(/^https?:\/\//, '').replace(/\/$/, '');
      const normalizedPE = peFirmWebsite?.toLowerCase().replace(/^https?:\/\//, '').replace(/\/$/, '');
      if (normalizedPE && normalizedUrl === normalizedPE) return 'pe_firm_website';
      return 'platform_website';
    };

    // Helper to process batch results with provenance tracking
    const processBatchResults = (results: PromiseSettledResult<{ name: string; result: AIExtractionResult; url: string | null | undefined }>[]) => {
      for (const settled of results) {
        if (settled.status === 'fulfilled') {
          const { name, result, url } = settled.value;
          const sourceType = getSourceType(url);

          if (result.error?.code === 'payment_required' || result.error?.code === 'rate_limited') {
            if (!billingError) billingError = result.error;
            continue;
          }

          if (result.data) {
            // Special handling for PE Intelligence: filter out thesis fields
            let dataToMerge = result.data;
            if (name === 'pe_intelligence') {
              const { thesis_summary, strategic_priorities, thesis_confidence, ...safeData } = result.data;
              if (thesis_summary || strategic_priorities || thesis_confidence) {
                console.warn('WARNING: PE Intelligence returned thesis fields - discarding (transcript-only)');
              }
              dataToMerge = safeData;
            }

            // Track source type per field and validate provenance BEFORE merging
            const acceptedFields: string[] = [];
            for (const [field, value] of Object.entries(dataToMerge)) {
              if (value === null || value === undefined) continue;

              const mappedField = FIELD_TO_COLUMN_MAP[field] || field;
              const validation = validateFieldProvenance(mappedField, sourceType);

              if (!validation.allowed) {
                console.error(`ðŸš« ${validation.reason}`);
                provenanceViolations.push(validation.reason!);
                continue;
              }

              allExtracted[field] = value;
              fieldSourceMap[mappedField] = sourceType;
              acceptedFields.push(field);
            }

            if (acceptedFields.length > 0) {
              promptsSuccessful++;
              if (result.usage) {
                totalInputTokens += result.usage.inputTokens || 0;
                totalOutputTokens += result.usage.outputTokens || 0;
              }
              evidenceRecords.push({
                type: 'website',
                source_type: sourceType,
                url,
                extracted_at: timestamp,
                fields_extracted: acceptedFields,
              });
            }
          }
        } else {
          console.error(`Prompt failed:`, settled.reason);
        }
      }
    };

    // Build extraction tasks as thunks so we can control concurrency
    type ExtractionResult = { name: string; result: AIExtractionResult; url: string | null | undefined };
    const allTasks: (() => Promise<ExtractionResult>)[] = [];

    if (platformContent) {
      allTasks.push(
        () => extractBusinessOverview(platformContent, geminiApiKey, _rateLimitConfig).then(r => ({ name: 'business', result: r, url: platformWebsite })),
        () => extractGeography(platformContent, geminiApiKey, _rateLimitConfig).then(r => ({ name: 'geography', result: validateGeography(r), url: platformWebsite })),
        () => extractCustomerProfile(platformContent, geminiApiKey, _rateLimitConfig).then(r => ({ name: 'customer', result: r, url: platformWebsite })),
        () => extractPEIntelligence(platformContent, geminiApiKey, _rateLimitConfig).then(r => ({ name: 'pe_intelligence', result: r, url: platformWebsite })),
      );
    } else if (peContent) {
      // No platform content â€” only extract geography from PE site
      console.log('Platform website unavailable â€” extracting geographic_footprint/service_regions ONLY from PE firm website');
      allTasks.push(
        () => extractGeography(peContent, geminiApiKey, _rateLimitConfig).then(r => {
          const validated = validateGeography(r);
          if (validated?.data) {
            delete validated.data.hq_city;
            delete validated.data.hq_state;
            delete validated.data.hq_country;
            delete validated.data.hq_region;
            delete validated.data.operating_locations;
            delete validated.data.service_regions;
          }
          return { name: 'geography', result: validated, url: peFirmWebsite };
        }),
      );
    }

    if (peContent) {
      allTasks.push(
        () => extractPEIntelligence(peContent, geminiApiKey, _rateLimitConfig).then(r => ({ name: 'pe_intelligence', result: r, url: peFirmWebsite })),
      );
    }

    if (allTasks.length > 0) {
      promptsRun += allTasks.length;

      // Execute in batches of AI_CONCURRENCY to prevent rate limit storms
      const allResults: PromiseSettledResult<ExtractionResult>[] = [];
      for (let b = 0; b < allTasks.length; b += BUYER_AI_CONCURRENCY) {
        const batch = allTasks.slice(b, b + BUYER_AI_CONCURRENCY);
        console.log(`Running AI batch ${Math.floor(b / BUYER_AI_CONCURRENCY) + 1}/${Math.ceil(allTasks.length / BUYER_AI_CONCURRENCY)} (${batch.length} prompts)`);
        const batchResults = await Promise.allSettled(batch.map(fn => fn()));
        allResults.push(...batchResults);
      }

      processBatchResults(allResults);
      console.log(`All prompts complete: ${promptsSuccessful}/${promptsRun} successful`);
    }

    console.log(`Extraction complete: ${promptsSuccessful}/${promptsRun} prompts successful, ${Object.keys(allExtracted).length} fields extracted`);

    // Handle billing errors with partial save
    if (billingError) {
      const be = billingError as unknown as { code: string; message: string };
      const fieldsExtracted = Object.keys(allExtracted).length;
      if (fieldsExtracted > 0) {
        const partialUpdate = buildBuyerUpdateObject(buyer, allExtracted, hasTranscriptSource, existingSources, evidenceRecords, fieldSourceMap);
        await supabase.from('remarketing_buyers').update(partialUpdate).eq('id', buyerId);
      }

      if (be.code === 'rate_limited' && fieldsExtracted > 0) {
        console.log(`Partial enrichment saved despite rate limit: ${fieldsExtracted} fields for buyer ${buyerId}`);
        return new Response(
          JSON.stringify({
            success: true,
            fieldsUpdated: fieldsExtracted,
            sources,
            warnings: [...warnings, `Partial enrichment: ${promptsSuccessful}/${promptsRun} prompts completed before rate limit. Data was saved.`],
            extractionDetails: {
              promptsRun,
              promptsSuccessful,
              platformScraped: !!platformContent,
              peFirmScraped: !!peContent,
              rateLimited: true,
            },
          }),
          { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
        );
      }

      return new Response(
        JSON.stringify({
          success: false,
          error: be.message,
          error_code: be.code,
          fieldsUpdated: fieldsExtracted,
          recoverable: be.code === 'rate_limited',
        }),
        { status: be.code === 'payment_required' ? 402 : 429, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    // ========================================================================
    // SAVE TO DATABASE
    // ========================================================================

    const updateData = buildBuyerUpdateObject(buyer, allExtracted, hasTranscriptSource, existingSources, evidenceRecords, fieldSourceMap);

    const { error: updateError } = await supabase
      .from('remarketing_buyers')
      .update(updateData)
      .eq('id', buyerId);

    if (updateError) {
      console.error('Database update error:', updateError);
      return new Response(
        JSON.stringify({ success: false, error: 'Failed to save enrichment', details: updateError.message }),
        { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
      );
    }

    const fieldsUpdated = Object.keys(updateData).length - 2; // Exclude metadata fields
    console.log(`Successfully enriched buyer ${buyerId}: ${fieldsUpdated} fields updated`);

    // Cost tracking: log aggregate AI usage (non-blocking)
    logAICallCost(supabase, 'enrich-buyer', 'gemini', BUYER_AI_CONFIG.model,
      {
        inputTokens: totalInputTokens > 0 ? totalInputTokens : promptsRun * 12000,
        outputTokens: totalOutputTokens > 0 ? totalOutputTokens : promptsSuccessful * 800,
      },
      undefined, { buyerId, promptsRun, promptsSuccessful }
    ).catch(() => {});

    // Log provenance violations as prominent warnings
    if (provenanceViolations.length > 0) {
      console.error(`âš ï¸ PROVENANCE REPORT: ${provenanceViolations.length} violation(s) blocked during enrichment of buyer ${buyerId}:`);
      for (const v of provenanceViolations) {
        console.error(`  â†’ ${v}`);
      }
      warnings.push(...provenanceViolations.map(v => `[BLOCKED] ${v}`));
    }

    return new Response(
      JSON.stringify({
        success: true,
        fieldsUpdated,
        sources,
        warnings: warnings.length > 0 ? warnings : undefined,
        provenanceViolations: provenanceViolations.length > 0 ? provenanceViolations : undefined,
        extractionDetails: {
          promptsRun,
          promptsSuccessful,
          platformScraped: !!platformContent,
          peFirmScraped: !!peContent,
          provenanceViolationsBlocked: provenanceViolations.length,
        },
      }),
      { headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );

  } catch (error) {
    console.error('Enrichment error:', error);
    return new Response(
      JSON.stringify({ success: false, error: error instanceof Error ? error.message : 'Unknown error' }),
      { status: 500, headers: { ...corsHeaders, 'Content-Type': 'application/json' } }
    );
  }
});
